{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Transfer Learning</h1>\r\n",
    "Guide from: https://medium.com/@saitejaponugoti/transfer-learning-for-deep-neural-networks-using-tensorflow-d628e454e9e5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_datasets as tfds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "(train_raw, validation_raw, test_raw), metadata = tfds.load(\r\n",
    "    'cats_vs_dogs',\r\n",
    "    split=['train[:20%]', 'train[80%:85%]', 'train[90%:95%]'],\r\n",
    "    with_info=True,\r\n",
    "    as_supervised=True,\r\n",
    "    shuffle_files=True,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# any image size can be selected\r\n",
    "IMG_SIZE = 160 # All images will be resized to 224x224x3\r\n",
    "\r\n",
    "def image_formatting(image, label):\r\n",
    "    # converting pixel values to float type\r\n",
    "    image = tf.cast(image, tf.float32)\r\n",
    "    # normalising the data to be in range of -1 tp +1 255/2 = 127.5\r\n",
    "    image = (image/127.5) - 1\r\n",
    "    # resizing all images to a shape of 224x*224*3\r\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\r\n",
    "    return image, label\r\n",
    "    \r\n",
    "#converting all images to same shape and formatting them for quicker training of the model\r\n",
    "train = train_raw.map(image_formatting)\r\n",
    "validation = validation_raw.map(image_formatting)\r\n",
    "test = test_raw.map(image_formatting)\r\n",
    "\r\n",
    "# select required batch_size and suffle_buffer_size\r\n",
    "BATCH_SIZE = 128\r\n",
    "SHUFFLE_BUFFER_SIZE = 1000\r\n",
    "\r\n",
    "# creating suffled training batches batches and shuffling for training and \r\n",
    "# testing data doesn't matter as they are only used for evaluating the model \r\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
    "validation_batches = validation.batch(BATCH_SIZE)\r\n",
    "test_batches = test.batch(BATCH_SIZE)\r\n",
    "\r\n",
    "# checking the batch formation\r\n",
    "for image_batch, label_batch in train_batches.take(1):\r\n",
    "   pass\r\n",
    "\r\n",
    "# expected size (batch_size,160*160*3)\r\n",
    "image_batch.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([128, 160, 160, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# defining the input image size \r\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\r\n",
    "\r\n",
    "# Create the base model from the MobileNet V2\r\n",
    "# Choosing include_top = False , so that we can define our own classification layer\r\n",
    "# Choosing weight = None to select a non-pretrained model\r\n",
    "original_model = tf.keras.applications.MobileNetV2(\r\n",
    "                                        input_shape=IMG_SHAPE,\r\n",
    "                                        include_top=False,\r\n",
    "                                        weights=None,\r\n",
    "                                        )\r\n",
    "\r\n",
    "# Checking the model summary and number of tunable prameters\r\n",
    "# original_model.summary()\r\n",
    "\r\n",
    "# lets observe what is the feature vector shape given by the model\r\n",
    "# without the inbuilt classification layer\r\n",
    "feature_batch = original_model(image_batch)\r\n",
    "\r\n",
    "# Setting up that the model can be trained\r\n",
    "# expected shape (batch_size, 5, 5, 1280)\r\n",
    "original_model.trainable = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# building a GlobalAveragePooling2D\r\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n",
    "feature_batch_average = global_average_layer(feature_batch)\r\n",
    "print(feature_batch_average.shape)\r\n",
    "\r\n",
    "# building a Dense output layer\r\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation = \"sigmoid\")\r\n",
    "prediction_batch = prediction_layer(feature_batch_average)\r\n",
    "print(prediction_batch.shape)\r\n",
    "\r\n",
    "# Adding GlobalAveragePooling2D and Dense output layer to form a model\r\n",
    "model_non_pretrained = tf.keras.Sequential([\r\n",
    "  original_model,\r\n",
    "  global_average_layer,\r\n",
    "  prediction_layer\r\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 1280)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Compiling the mode by including optimizer, loss function and metrics \r\n",
    "base_learning_rate = 0.0001\r\n",
    "model_non_pretrained.compile(\r\n",
    "    optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate), \r\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \r\n",
    "    metrics=['accuracy']\r\n",
    "    )\r\n",
    "\r\n",
    "# Number of epochs\r\n",
    "epochs_number = 10\r\n",
    "# Fitting the model with train batches\r\n",
    "history = model_non_pretrained.fit(\r\n",
    "    train_batches,\r\n",
    "    epochs=epochs_number ,verbose = 1,\r\n",
    "    validation_data = validation_batches\r\n",
    "    )\r\n",
    "\r\n",
    "# Evaluating teh accuracy on test set batches\r\n",
    "results = model_non_pretrained.evaluate(test_batches)\r\n",
    "print('test loss, test acc:', results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:5016: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37/37 [==============================] - 36s 713ms/step - loss: 0.6804 - accuracy: 0.5681 - val_loss: 0.6933 - val_accuracy: 0.4815\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 25s 660ms/step - loss: 0.6379 - accuracy: 0.6316 - val_loss: 0.6934 - val_accuracy: 0.4815\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 29s 759ms/step - loss: 0.5878 - accuracy: 0.6883 - val_loss: 0.6939 - val_accuracy: 0.4815\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 28s 729ms/step - loss: 0.5371 - accuracy: 0.7330 - val_loss: 0.6946 - val_accuracy: 0.4815\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 27s 712ms/step - loss: 0.4593 - accuracy: 0.7784 - val_loss: 0.6955 - val_accuracy: 0.4815\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 27s 706ms/step - loss: 0.3979 - accuracy: 0.8160 - val_loss: 0.6963 - val_accuracy: 0.4815\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 30s 783ms/step - loss: 0.3413 - accuracy: 0.8504 - val_loss: 0.7002 - val_accuracy: 0.4815\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 27s 705ms/step - loss: 0.2757 - accuracy: 0.8835 - val_loss: 0.7058 - val_accuracy: 0.4815\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 26s 681ms/step - loss: 0.2880 - accuracy: 0.8775 - val_loss: 0.7092 - val_accuracy: 0.4815\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 25s 668ms/step - loss: 0.2258 - accuracy: 0.9035 - val_loss: 0.7157 - val_accuracy: 0.4815\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.7108 - accuracy: 0.4953\n",
      "test loss, test acc: [0.710771381855011, 0.49527084827423096]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\r\n",
    "frozen_model = tf.keras.applications.MobileNetV2(\r\n",
    "    input_shape=IMG_SHAPE,\r\n",
    "    include_top=False,\r\n",
    "    weights='imagenet'\r\n",
    "    )\r\n",
    "\r\n",
    "# lets observe what is the feature vector shape given by the model\r\n",
    "# without the inbuilt classification layer\r\n",
    "feature_batch = frozen_model(image_batch)\r\n",
    "# print(feature_batch.shape)\r\n",
    "\r\n",
    "# Freezing all the layers of the model\r\n",
    "frozen_model.trainable = False\r\n",
    "\r\n",
    "# Checking the number of trainable parametres Expected : 0\r\n",
    "# frozen_model.summary()\r\n",
    "\r\n",
    "# Adding GlobalAveragePooling2D and Dense output layer to form a model\r\n",
    "model_frozen_pretrained = tf.keras.Sequential([\r\n",
    "  frozen_model,\r\n",
    "  global_average_layer,\r\n",
    "  prediction_layer\r\n",
    "])\r\n",
    "\r\n",
    "# Compiling the mode by including optimizer, loss function and metrics \r\n",
    "model_frozen_pretrained.compile(\r\n",
    "    optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate), \r\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \r\n",
    "    metrics=['accuracy']\r\n",
    "    )\r\n",
    "\r\n",
    "# Number of epochs\r\n",
    "epochs_number = 10\r\n",
    "# Fitting the model with train batches\r\n",
    "history_unfreeze = model_frozen_pretrained.fit(\r\n",
    "    train_batches,\r\n",
    "    epochs=epochs_number ,verbose = 1,\r\n",
    "    validation_data = validation_batches\r\n",
    "    )\r\n",
    "\r\n",
    "# Evaluating the accuracy on test set batches\r\n",
    "results = model_frozen_pretrained.evaluate(test_batches)\r\n",
    "print('test loss, test acc:', results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n",
      "9412608/9406464 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:5016: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37/37 [==============================] - 11s 237ms/step - loss: 0.5164 - accuracy: 0.7732 - val_loss: 0.4183 - val_accuracy: 0.8461\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 9s 220ms/step - loss: 0.3737 - accuracy: 0.8792 - val_loss: 0.3135 - val_accuracy: 0.9097\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 9s 226ms/step - loss: 0.2833 - accuracy: 0.9211 - val_loss: 0.2407 - val_accuracy: 0.9381\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.2209 - accuracy: 0.9428 - val_loss: 0.1911 - val_accuracy: 0.9493\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.1778 - accuracy: 0.9568 - val_loss: 0.1566 - val_accuracy: 0.9587\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.1479 - accuracy: 0.9637 - val_loss: 0.1322 - val_accuracy: 0.9656\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 9s 227ms/step - loss: 0.1266 - accuracy: 0.9697 - val_loss: 0.1148 - val_accuracy: 0.9690\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.1114 - accuracy: 0.9727 - val_loss: 0.1025 - val_accuracy: 0.9733\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 9s 218ms/step - loss: 0.1000 - accuracy: 0.9744 - val_loss: 0.0933 - val_accuracy: 0.9742\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 9s 224ms/step - loss: 0.0915 - accuracy: 0.9753 - val_loss: 0.0864 - val_accuracy: 0.9742\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.0962 - accuracy: 0.9716\n",
      "test loss, test acc: [0.09617700427770615, 0.9716250896453857]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# building a GlobalAveragePooling2D\r\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n",
    "feature_batch_average = global_average_layer(feature_batch)\r\n",
    "\r\n",
    "# building a Dense output layer\r\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation = \"sigmoid\")\r\n",
    "prediction_batch = prediction_layer(feature_batch_average)\r\n",
    "base_learning_rate = 0.0001\r\n",
    "# Create the base model from the pre-trained model MobileNet V2\r\n",
    "unfrozen_model = tf.keras.applications.MobileNetV2(\r\n",
    "    input_shape=IMG_SHAPE,\r\n",
    "    include_top=False,\r\n",
    "    weights='imagenet'\r\n",
    "    )\r\n",
    "# lets observe what is the feature vector shape given by the model\r\n",
    "# without the inbuilt classification layer\r\n",
    "feature_batch = unfrozen_model(image_batch)\r\n",
    "# print(feature_batch.shape)\r\n",
    "\r\n",
    "# Freezing all the layers of the model below 100th layer\r\n",
    "freeze_till_layer = 100\r\n",
    "\r\n",
    "# Freeze all the layers before the `ffreeze_till_layer` layer\r\n",
    "for layer in unfrozen_model.layers[:freeze_till_layer]:\r\n",
    "  layer.trainable =  False\r\n",
    "\r\n",
    "# Adding GlobalAveragePooling2D and Dense output layer to form a model\r\n",
    "model_unfrozen_pretrained = tf.keras.Sequential([\r\n",
    "  unfrozen_model,\r\n",
    "  global_average_layer,\r\n",
    "  prediction_layer\r\n",
    "])\r\n",
    "\r\n",
    "\r\n",
    "# Compiling the mode by including optimizer, loss function and metrics \r\n",
    "model_unfrozen_pretrained.compile(\r\n",
    "    optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate), \r\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \r\n",
    "    metrics=['accuracy']\r\n",
    "    )\r\n",
    "\r\n",
    "# Number of epochs\r\n",
    "epochs_number = 10\r\n",
    "# Fitting the model with train batches\r\n",
    "history_unfreeze = model_unfrozen_pretrained.fit(\r\n",
    "    train_batches,\r\n",
    "    epochs=epochs_number ,verbose = 1,\r\n",
    "    validation_data = validation_batches\r\n",
    "    )\r\n",
    "\r\n",
    "# Evaluating the accuracy on test set batches\r\n",
    "results = model_unfrozen_pretrained.evaluate(test_batches)\r\n",
    "print('test loss, test acc:', results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:5016: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37/37 [==============================] - 17s 308ms/step - loss: 0.1280 - accuracy: 0.9506 - val_loss: 0.0660 - val_accuracy: 0.9785\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 11s 271ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.0798 - val_accuracy: 0.9768\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 11s 287ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 0.0737 - val_accuracy: 0.9759\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 11s 281ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0858 - val_accuracy: 0.9768\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 11s 280ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0772 - val_accuracy: 0.9828\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 11s 278ms/step - loss: 7.3456e-04 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 11s 277ms/step - loss: 2.4960e-04 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9802\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 11s 278ms/step - loss: 1.7041e-04 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 0.9819\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 10s 269ms/step - loss: 8.8728e-05 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9828\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 10s 268ms/step - loss: 5.1697e-05 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 0.9794\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.1409 - accuracy: 0.9751\n",
      "test loss, test acc: [0.14093244075775146, 0.9750645160675049]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "a145704f143a95e481fb6a6107756dde8979ba8618f7999081d4774de4fc6704"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}